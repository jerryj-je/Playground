{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e0500c1-23f0-4702-8f8a-295c7497eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and groups in the H5 file:\n",
      "float_columns\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# File name\n",
    "file_name = \"final_updated_features.h5\"\n",
    "\n",
    "# Step 1: Open the H5 file and explore its structure\n",
    "with h5py.File(file_name, 'r') as h5file:\n",
    "    print(\"Datasets and groups in the H5 file:\")\n",
    "    h5file.visit(print)  # Print all groups and datasets in the filefinal_updated_features.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdd9404b-03a7-4080-96d2-d6fc28a68730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: float_columns\n",
      "Shape: (25158056, 8)\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Choose a dataset to explore (replace 'your_dataset_name' with the actual dataset name after inspecting the file)\n",
    "dataset_name = 'float_columns'  # Replace this with the actual dataset name\n",
    "\n",
    "with h5py.File(file_name, 'r') as h5file:\n",
    "    if dataset_name in h5file:\n",
    "        data = h5file[dataset_name]\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "        print(f\"Data type: {data.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "974786ed-2b37-4130-9a83-c5e91e0037ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'combined_data' has shape: (25158056, 8)\n",
      "[[7413.        775.        775.       ...    9.945374  320.\n",
      "   180.      ]\n",
      " [7413.        775.        775.       ...    9.945374  320.\n",
      "   180.      ]\n",
      " [7413.        775.        775.       ...    9.945374  320.\n",
      "   180.      ]\n",
      " ...\n",
      " [2479.       1575.       2375.       ...   11.548871 -128.\n",
      "   180.      ]\n",
      " [2479.       1575.       2375.       ...   11.548871 -128.\n",
      "   180.      ]\n",
      " [2479.       1575.       2375.       ...   11.548871 -128.\n",
      "   180.      ]]\n"
     ]
    }
   ],
   "source": [
    "##file_name = \"combined_with_flight_id.h5\"\n",
    "# File path\n",
    "file_name = \"final_updated_features.h5\"\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_name, 'r') as hf:\n",
    "    # Check if the dataset exists\n",
    "    if 'float_columns' in hf:\n",
    "        dataset = hf['float_columns']\n",
    "        \n",
    "        # Print the shape of the dataset\n",
    "        print(f\"Dataset 'combined_data' has shape: {dataset.shape}\")\n",
    "        \n",
    "        # Print the first few rows of the data\n",
    "        print(dataset[:10000])  \n",
    "\n",
    "    else:\n",
    "        print(\"'final_combined_with_flight_id' dataset not found in the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48c4cf99-ef08-494f-9b7e-b738ef8b6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "file_name = \"final_updated_features.h5\"  # Replace with your actual file path\n",
    "with h5py.File(file_name, 'r') as h5_file:\n",
    "    dataset = h5_file['float_columns'][:]  # Load the entire dataset into memory\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1573a1fc-2d6b-4360-a378-d72f335755c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0      1      2     3          4         5      6      7\n",
      "0  7413.0  775.0  775.0  91.0  53.473297  9.945374  320.0  180.0\n",
      "1  7413.0  775.0  775.0  91.0  53.473297  9.945374  320.0  180.0\n",
      "2  7413.0  775.0  775.0  91.0  53.473297  9.945374  320.0  180.0\n",
      "3  7413.0  775.0  775.0  91.0  53.473297  9.945374  320.0  180.0\n",
      "4  7413.0  775.0  775.0  91.0  53.473297  9.945374  320.0  180.0\n",
      "                0       1       2      3          4          5    6      7\n",
      "25158051  21105.0  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0\n",
      "25158052  21105.0  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0\n",
      "25158053  21105.0  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0\n",
      "25158054  21105.0  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0\n",
      "25158055  21105.0  2775.0  3250.0  123.0  47.617264  10.281898  0.0  180.0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63fb7b5f-54a6-45a9-9077-794ea3a29997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['flight_id', 'altitude', 'geoaltitude', 'groundspeed', 'latitude',\n",
      "       'longitude', 'vertical_rate', 'compute_gs'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assign custom column names based on the list you provided\n",
    "column_names = ['flight_id', 'altitude', 'geoaltitude', 'groundspeed', 'latitude', 'longitude', 'vertical_rate', 'compute_gs']\n",
    "\n",
    "# Assign these column names to the DataFrame\n",
    "df.columns = column_names\n",
    "\n",
    "# Check if the column names are assigned correctly\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad32f609-8ff8-4280-bdfe-6e298bd22fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   flight_id  altitude  geoaltitude  groundspeed  vertical_rate  compute_gs  \\\n",
      "0     7413.0     775.0        775.0         91.0          320.0       180.0   \n",
      "1     7413.0     775.0        775.0         91.0          320.0       180.0   \n",
      "2     7413.0     775.0        775.0         91.0          320.0       180.0   \n",
      "3     7413.0     775.0        775.0         91.0          320.0       180.0   \n",
      "4     7413.0     775.0        775.0         91.0          320.0       180.0   \n",
      "\n",
      "          x         y        z  \n",
      "0  0.586253  0.102796  0.80358  \n",
      "1  0.586253  0.102796  0.80358  \n",
      "2  0.586253  0.102796  0.80358  \n",
      "3  0.586253  0.102796  0.80358  \n",
      "4  0.586253  0.102796  0.80358  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 'latitude' and 'longitude' columns in degrees\n",
    "df['lat_rad'] = np.radians(df['latitude'])  # Convert latitude to radians\n",
    "df['lon_rad'] = np.radians(df['longitude'])  # Convert longitude to radians\n",
    "\n",
    "# Compute x, y, z coordinates\n",
    "df['x'] = np.cos(df['lat_rad']) * np.cos(df['lon_rad'])\n",
    "df['y'] = np.cos(df['lat_rad']) * np.sin(df['lon_rad'])\n",
    "df['z'] = np.sin(df['lat_rad'])\n",
    "\n",
    "# Drop original latitude, longitude, and intermediate radian columns\n",
    "df.drop(columns=['latitude', 'longitude', 'lat_rad', 'lon_rad'], inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61ac0e6a-ea64-4718-9f21-18d442937ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"flight_id\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42b237f2-1afa-4752-9c37-8ab0894de710",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the flight IDs after sorting\n",
    "flight_ids_after = df[\"flight_id\"].values  # Get the flight IDs after sorting\n",
    "\n",
    "# Line plot to confirm sorting of flight_ids\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(flight_ids_after, linestyle='-', marker='', color='green', label='Sorted Flight IDs')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Flight ID')\n",
    "plt.title('Flight IDs Distribution After Sorting')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for clarity (after sorting)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(range(len(flight_ids_after)), flight_ids_after, color='red', s=1, label='Sorted Flight IDs')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Flight ID')\n",
    "plt.title('Flight IDs Distribution After Sorting (Scatter)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfd99a3b-7451-4bdb-9233-46279041454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"flight_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91e81610-b65b-463a-8189-3ce06cd0bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after sorting and removing the flight_id column:\n",
      "       altitude  geoaltitude  groundspeed  vertical_rate  compute_gs  \\\n",
      "11797    1400.0       1375.0        132.0            0.0  180.000000   \n",
      "88921     450.0        425.0        116.0           64.0  180.000000   \n",
      "88922     475.0        425.0        115.0          192.0  247.185776   \n",
      "88923     475.0        450.0        115.0          192.0  180.000000   \n",
      "88924     475.0        450.0        115.0          192.0  180.000000   \n",
      "\n",
      "              x         y         z  \n",
      "11797  0.586926  0.104000  0.802933  \n",
      "88921  0.586205  0.103102  0.803575  \n",
      "88922  0.586226  0.103064  0.803565  \n",
      "88923  0.586226  0.103064  0.803565  \n",
      "88924  0.586226  0.103064  0.803565  \n"
     ]
    }
   ],
   "source": [
    "print(\"Data after sorting and removing the flight_id column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd94b342-08ae-4ab0-a93b-fff8b1455ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25158056, 8)\n",
      "y shape: (25158056, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X = df.values  # Use all columns as input\n",
    "y = df.iloc[:, [0, 5, 6, 7]].values  # Select columns 1, 4, and 5 (0-based index)\n",
    "\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "#X = df.iloc[:, 3:].values  # Use all columns except the first 3 as input\n",
    "#y = df.iloc[:, :3].values  # Use the first 3 columns as output\n",
    "\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2858e90-b0ec-4f5d-bbdb-b07c9bd0a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20126444, 8)\n",
      "y_train shape: (20126444, 4)\n",
      "X_test shape: (5031612, 8)\n",
      "y_test shape: (5031612, 4)\n"
     ]
    }
   ],
   "source": [
    "## splitting into train and test\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(df) * train_ratio)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de318b1e-4082-4d50-bcd9-b3e5da5f3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20126384, 8)\n",
      "y_train shape: (20126384, 4)\n",
      "X_test shape: (5031552, 8)\n",
      "y_test shape: (5031552, 4)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_target_steps = 60  # Predict 60 timesteps into the future\n",
    "\n",
    "# Initialize lists for inputs and outputs\n",
    "X_train_new = []\n",
    "y_train_new = []\n",
    "\n",
    "# Create training data\n",
    "for i in range(len(X_train) - n_target_steps):\n",
    "    # Current timestep as input\n",
    "    X_train_new.append(X_train[i, :])  # Take the current timestep (single row)\n",
    "    # Value 60 timesteps into the future as output\n",
    "    y_train_new.append(y_train[i + n_target_steps, :])  # Predict 60 timesteps ahead\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = np.array(X_train_new)\n",
    "y_train = np.array(y_train_new)\n",
    "\n",
    "# Do the same for the test set\n",
    "X_test_new = []\n",
    "y_test_new = []\n",
    "\n",
    "for i in range(len(X_test) - n_target_steps):\n",
    "    X_test_new.append(X_test[i, :])\n",
    "    y_test_new.append(y_test[i + n_target_steps, :])\n",
    "\n",
    "X_test = np.array(X_test_new)\n",
    "y_test= np.array(y_test_new)\n",
    "\n",
    "# Check shapes\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (samples, features)\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (samples, target_features)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33401afc-cd1d-420a-874f-f21b86b5b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (20126384, 8)\n",
      "y_train_scaled shape: (20126384, 4)\n",
      "X_test_scaled shape: (5031552, 8)\n",
      "y_test_scaled shape: (5031552, 4)\n",
      "[[ 0.0203125  -0.9603036  -0.8730159  -0.02512562  0.          0.75773746\n",
      "   0.24428366  0.94557846]\n",
      " [ 0.01289063 -0.9750925  -0.8884079  -0.02311557  0.          0.75591415\n",
      "   0.24301016  0.9462868 ]\n",
      " [ 0.01308594 -0.9750925  -0.8893699  -0.01909547  0.3732543   0.7559672\n",
      "   0.24295574  0.9462753 ]\n",
      " [ 0.01308594 -0.97470325 -0.8893699  -0.01909547  0.          0.7559672\n",
      "   0.24295574  0.9462753 ]\n",
      " [ 0.01308594 -0.97470325 -0.8893699  -0.01909547  0.          0.7559672\n",
      "   0.24295574  0.9462753 ]]\n",
      "[[0.01171875 0.7560647  0.24266607 0.9462732 ]\n",
      " [0.01171875 0.7560647  0.24266607 0.9462732 ]\n",
      " [0.01171875 0.7560647  0.24266607 0.9462732 ]\n",
      " [0.01230469 0.75619906 0.24253727 0.9462433 ]\n",
      " [0.0125     0.7561955  0.2425836  0.9462398 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create MinMaxScaler objects\n",
    "scaler_X_altitude = MinMaxScaler(feature_range=(0, 1))  # For altitude to scale between 0 and 1\n",
    "scaler_X_other = MinMaxScaler(feature_range=(-1, 1))  # For other features to scale between -1 and 1\n",
    "scaler_y_altitude = MinMaxScaler(feature_range=(0, 1))  # For output altitude to scale between 0 and 1\n",
    "scaler_y_other = MinMaxScaler(feature_range=(-1, 1))  # For other output columns to scale between -1 and 1\n",
    "\n",
    "# Identify the altitude column index\n",
    "altitude_index = 0  # Change this based on your dataset\n",
    "\n",
    "# Separate the altitude column from X (features)\n",
    "altitude_train_X = X_train[:, altitude_index].reshape(-1, 1)\n",
    "X_train_other = X_train[:, [i for i in range(X_train.shape[1]) if i != altitude_index]]\n",
    "\n",
    "altitude_test_X = X_test[:, altitude_index].reshape(-1, 1)\n",
    "X_test_other = X_test[:, [i for i in range(X_test.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# Separate the altitude column from y (target)\n",
    "altitude_train_y = y_train[:, altitude_index].reshape(-1, 1)\n",
    "y_train_other = y_train[:, [i for i in range(y_train.shape[1]) if i != altitude_index]]\n",
    "\n",
    "altitude_test_y = y_test[:, altitude_index].reshape(-1, 1)\n",
    "y_test_other = y_test[:, [i for i in range(y_test.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# Scale the altitude feature in X to [0, 1]\n",
    "altitude_train_X_scaled = scaler_X_altitude.fit_transform(altitude_train_X)\n",
    "altitude_test_X_scaled = scaler_X_altitude.transform(altitude_test_X)\n",
    "\n",
    "# Scale the other features in X to [-1, 1]\n",
    "X_train_other_scaled = scaler_X_other.fit_transform(X_train_other)\n",
    "X_test_other_scaled = scaler_X_other.transform(X_test_other)\n",
    "\n",
    "# Scale the altitude in y to [0, 1]\n",
    "altitude_train_y_scaled = scaler_y_altitude.fit_transform(altitude_train_y)\n",
    "altitude_test_y_scaled = scaler_y_altitude.transform(altitude_test_y)\n",
    "\n",
    "# Scale the other columns in y to [-1, 1]\n",
    "y_train_other_scaled = scaler_y_other.fit_transform(y_train_other)\n",
    "y_test_other_scaled = scaler_y_other.transform(y_test_other)\n",
    "\n",
    "# Combine the scaled altitude with the scaled other features in X\n",
    "X_train = np.hstack([altitude_train_X_scaled, X_train_other_scaled])\n",
    "X_test = np.hstack([altitude_test_X_scaled, X_test_other_scaled])\n",
    "\n",
    "# Combine the scaled altitude with the scaled other columns in y\n",
    "y_train= np.hstack([altitude_train_y_scaled, y_train_other_scaled])\n",
    "y_test = np.hstack([altitude_test_y_scaled, y_test_other_scaled])\n",
    "\n",
    "# Check the shapes after scaling (should remain the same)\n",
    "print(\"X_train_scaled shape:\", X_train.shape)\n",
    "print(\"y_train_scaled shape:\", y_train.shape)\n",
    "print(\"X_test_scaled shape:\", X_test.shape)\n",
    "print(\"y_test_scaled shape:\", y_test.shape)\n",
    "\n",
    "# Optional: Print the first few rows of the scaled training data\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c761340-027c-4c37-af73-616f667ae8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after expansion: (20126384, 1, 8)\n",
      "X_test shape after expansion: (5031552, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "# Expand dimensions for LSTM input (samples, time_steps, features)\n",
    "X_train = np.expand_dims(X_train, axis=1)  # Shape becomes (samples, 1, features)\n",
    "X_test = np.expand_dims(X_test, axis=1)    \n",
    "\n",
    "print(\"X_train shape after expansion:\", X_train.shape)\n",
    "print(\"X_test shape after expansion:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0cef25c-aca1-458c-b732-69300e3bf9b8",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(y_train.shape[1])  # Output layer: Predict the 3 target columns\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b19f490a-ce5a-4d54-8e81-ea63ed90991f",
   "metadata": {},
   "source": [
    "#ading early stopping and chnaging relu to tanh, adding dropout layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(9, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Add dropout for regularization\n",
    "\n",
    "model.add(LSTM(9, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(9, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(y_train.shape[1])) \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitors the validation loss\n",
    "    patience=3,           # Number of epochs to wait for improvement before stopping\n",
    "    restore_best_weights=True  # Restores the best model weights after stopping\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    verbose=1, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=[early_stopping]  # Pass EarlyStopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12677c09-7238-47ab-bf99-5b743b989ada",
   "metadata": {},
   "source": [
    "#ading early stopping and chnaging relu to tanh, adding dropout layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(9, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.1))  # Add dropout for regularization\n",
    "\n",
    "model.add(LSTM(9, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(9, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(y_train.shape[1])) \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitors the validation loss\n",
    "    patience=3,           # Number of epochs to wait for improvement before stopping\n",
    "    restore_best_weights=True  # Restores the best model weights after stopping\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=128, \n",
    "    verbose=1, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=[early_stopping]  # Pass EarlyStopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61b6acfa-e4c7-4156-89a8-b3fb5bda3262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 2ms/step - loss: 7.8506e-04 - val_loss: 7.0979e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 2ms/step - loss: 9.8613e-05 - val_loss: 7.4285e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 2ms/step - loss: 9.7825e-05 - val_loss: 7.3671e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m314475/314475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 2ms/step - loss: 9.2767e-05 - val_loss: 7.7604e-05\n",
      "\u001b[1m157236/157236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 730us/step - loss: 5.3239e-05\n",
      "Test Loss: 7.0978443545755e-05\n"
     ]
    }
   ],
   "source": [
    "#ading early stopping and chnaging relu to tanh, adding dropout layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))  # Add dropout for regularization\n",
    "\n",
    "model.add(LSTM(50, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(50, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(y_train.shape[1])) \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitors the validation loss\n",
    "    patience=3,           # Number of epochs to wait for improvement before stopping\n",
    "    restore_best_weights=True  # Restores the best model weights after stopping\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=64, \n",
    "    verbose=1, \n",
    "    validation_data=(X_test, y_test), \n",
    "    callbacks=[early_stopping]  # Pass EarlyStopping callback\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "##0.0090 as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8680bd6b-abb4-43a4-892a-33da74888323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157236/157236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 787us/step\n",
      "Predicted vs Actual Values:\n",
      "Example 1:\n",
      "Predicted: [0.02802555 0.94260985 0.25375545 0.8797399 ]\n",
      "Actual: [0.02832031 0.9432599  0.25640523 0.8789666 ]\n",
      "------------------------------\n",
      "Example 2:\n",
      "Predicted: [0.02822705 0.9432615  0.2540965  0.8794498 ]\n",
      "Actual: [0.02832031 0.9432599  0.25640523 0.8789666 ]\n",
      "------------------------------\n",
      "Example 3:\n",
      "Predicted: [0.02822705 0.9432615  0.2540965  0.8794498 ]\n",
      "Actual: [0.02851563 0.9432822  0.2563979  0.87895876]\n",
      "------------------------------\n",
      "Example 4:\n",
      "Predicted: [0.02789884 0.9424933  0.25372615 0.8797879 ]\n",
      "Actual: [0.02832031 0.9433163  0.25638747 0.8789468 ]\n",
      "------------------------------\n",
      "Example 5:\n",
      "Predicted: [0.02822705 0.9432615  0.2540965  0.8794498 ]\n",
      "Actual: [0.02832031 0.9433163  0.25638747 0.8789468 ]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print a few examples of predictions and actual values\n",
    "num_examples = 5  # Number of examples to display\n",
    "print(\"Predicted vs Actual Values:\")\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Predicted: {y_pred[i]}\")\n",
    "    print(f\"Actual: {y_test[i]}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf4e92b8-5fe1-4f20-933f-079df91a7a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model,'model.pkl')\n",
    "#model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8079182-ef50-466c-a21f-bed888b2d959",
   "metadata": {},
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(window_size, len(train_data)):\n",
    "    # Input: Use all columns from the previous 60 rows (using .iloc for pandas)\n",
    "    X_train.append(train_data.iloc[i-60:i, :].values)  # All columns from the last 60 rows\n",
    "    \n",
    "    # Output: Columns 1, 2, and 3 (target columns)\n",
    "    y_train.append(train_data.iloc[i, 0:3].values)  # Columns 1, 2, and 3 as the target output\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee4ac28c-c598-485b-ab89-334e7c50ec5e",
   "metadata": {},
   "source": [
    "X_train = np.expand_dims(X_train.values, axis=1)  # Shape becomes (samples, 1, features)\n",
    "X_test = np.expand_dims(X_test.values, axis=1)    # Shape becomes (samples, 1, features)\n",
    "y_train = y_train.values  # Convert to NumPy array\n",
    "y_test = y_test.values    # Convert to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b1597-5435-4c28-a947-20f11df518d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "23093cfe-f8d1-4c72-95b0-2ee938aa098c",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Number of trees (n_estimators)\n",
    "n_estimators = 100\n",
    "\n",
    "# Initialize Random Forest model without training\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    warm_start=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_depth=10 \n",
    ")\n",
    "\n",
    "# Train the model one tree at a time with a progress bar\n",
    "for i in tqdm(range(1, n_estimators + 1), desc=\"Training Random Forest\"):\n",
    "    model.set_params(n_estimators=i)  # Incrementally increase the number of trees\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the latitude and longitude\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Optionally, print some of the predictions vs actual values\n",
    "print(\"Predictions vs Actual Values:\")\n",
    "print(pd.DataFrame({'Predicted Latitude': y_pred[:, 0], 'Actual Latitude': y_test[:, 0],\n",
    "                    'Predicted Longitude': y_pred[:, 1], 'Actual Longitude': y_test[:, 1]}).head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69ce174e-fed2-4fcf-90c6-12e43072f644",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np  # Import numpy to compute the square root\n",
    "\n",
    "# Initialize and train a regression model (Linear Regression)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the latitude and longitude\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Calculate the RMSE by taking the square root of MSE\n",
    "\n",
    "# Print the RMSE value\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Optionally, print some of the predictions vs actual values\n",
    "print(\"Predictions vs Actual Values:\")\n",
    "print(pd.DataFrame({'Predicted Latitude': y_pred[:, 1], 'Actual Latitude': y_test[:, 1],\n",
    "                    'Predicted Longitude': y_pred[:, 2], 'Actual Longitude': y_test[:, 2]}).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e6668c36-7693-480c-8589-a78d8c3c89d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files successfully loaded: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_frames = []  # List to hold DataFrames\n",
    "data_dir = \"/home/jovyan/data/data_test\"  # Data directory\n",
    "\n",
    "# Counter for successfully loaded files\n",
    "loaded_files_count = 0\n",
    "\n",
    "# Looping through all years and months\n",
    "for year in range(2017, 2022):  # From 2017 to 2021\n",
    "    for month in range(1, 13):  # Months from 1 to 12\n",
    "        # Construct the full file path\n",
    "        file_name = f\"test_set_{month}_{year}.csv\"  # The file name\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Load the data into a DataFrame\n",
    "            temp_data = pd.read_csv(file_path, sep=',')\n",
    "            \n",
    "            # Convert the timestamp column to datetime format\n",
    "            temp_data['timestamp'] = pd.to_datetime(temp_data['timestamp'])\n",
    "            \n",
    "            # Add the DataFrame to the list\n",
    "            data_frames.append(temp_data)\n",
    "            \n",
    "            # Increment the counter for successfully loaded files\n",
    "            loaded_files_count += 1\n",
    "        except FileNotFoundError:\n",
    "            # Skip the file\n",
    "            pass\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "merged_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Print the count of successfully loaded files\n",
    "print(f\"Number of CSV files successfully loaded: {loaded_files_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "57df7019-4b9b-4742-b72d-ed42bd8a4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0 registration  icao24 operator callsign  alert  \\\n",
      "0           108       79969       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "1           109       79970       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "2           110       79971       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "3           111       79972       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "4           112       79973       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "\n",
      "   altitude  count  geoaltitude  ...  squawk           timestamp      track  \\\n",
      "0    1275.0      2       1550.0  ...     NaN 2017-08-01 07:18:29  315.00000   \n",
      "1    1325.0      2       1600.0  ...     NaN 2017-08-01 07:18:30  293.19858   \n",
      "2    1325.0      2       1625.0  ...     NaN 2017-08-01 07:18:31  294.44397   \n",
      "3    1300.0      2       1625.0  ...     NaN 2017-08-01 07:18:32  294.44397   \n",
      "4    1300.0      2       1600.0  ...     NaN 2017-08-01 07:18:33  291.25052   \n",
      "\n",
      "   vertical_rate   flight_id   cumdist  compute_gs  compute_track month  \\\n",
      "0          576.0  3E0F76_969  0.000000   10.327583      270.00003     8   \n",
      "1          320.0  3E0F76_969  0.002869   10.327583      270.00003     8   \n",
      "2           64.0  3E0F76_969  0.002869    0.000000      180.00000     8   \n",
      "3           64.0  3E0F76_969  0.011918   32.575943      287.99356     8   \n",
      "4         -192.0  3E0F76_969  0.011918    0.000000      180.00000     8   \n",
      "\n",
      "   time_diff  \n",
      "0        NaN  \n",
      "1        1.0  \n",
      "2        1.0  \n",
      "3        1.0  \n",
      "4        1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "         Unnamed: 0.1  Unnamed: 0 registration  icao24 operator callsign  \\\n",
      "2177001       1452126      273576       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177002       1452127      273577       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177003       1452128      273578       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177004       1452129      273579       D-HZSD  3E167B      BMI    CHX29   \n",
      "2177005       1452130      273580       D-HZSD  3E167B      BMI    CHX29   \n",
      "\n",
      "         alert  altitude  count  geoaltitude  ...  squawk           timestamp  \\\n",
      "2177001  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:02   \n",
      "2177002  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:03   \n",
      "2177003  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:04   \n",
      "2177004  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:05   \n",
      "2177005  False    1150.0      7       1100.0  ...    34.0 2021-07-31 20:04:06   \n",
      "\n",
      "            track  vertical_rate   flight_id    cumdist  compute_gs  \\\n",
      "2177001  1.974934           64.0  CHX29_1765  11.544259   70.057920   \n",
      "2177002  1.992094         -128.0  CHX29_1765  11.544259    0.000000   \n",
      "2177003  1.992094            0.0  CHX29_1765  11.618588  267.582180   \n",
      "2177004  2.009554         -192.0  CHX29_1765  11.651598  118.840385   \n",
      "2177005  2.511363         -192.0  CHX29_1765  11.685999  123.841286   \n",
      "\n",
      "         compute_track month  time_diff  \n",
      "2177001       8.303264     7        1.0  \n",
      "2177002     180.000000     7        1.0  \n",
      "2177003       2.166752     7        1.0  \n",
      "2177004     360.000000     7        1.0  \n",
      "2177005       3.995433     7        1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.head())\n",
    "print(merged_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "934017b2-de88-439b-86f6-18e925a2136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0 registration  icao24 operator callsign  alert  \\\n",
      "0           108       79969       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "1           109       79970       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "2           110       79971       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "3           111       79972       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "4           112       79973       D-HXBA  3E0F76     ADAC    CHX32  False   \n",
      "\n",
      "   altitude  count  geoaltitude  ...  squawk           timestamp      track  \\\n",
      "0    1275.0      2       1550.0  ...     NaN 2017-08-01 07:18:29  315.00000   \n",
      "1    1325.0      2       1600.0  ...     NaN 2017-08-01 07:18:30  293.19858   \n",
      "2    1325.0      2       1625.0  ...     NaN 2017-08-01 07:18:31  294.44397   \n",
      "3    1300.0      2       1625.0  ...     NaN 2017-08-01 07:18:32  294.44397   \n",
      "4    1300.0      2       1600.0  ...     NaN 2017-08-01 07:18:33  291.25052   \n",
      "\n",
      "   vertical_rate           flight_id   cumdist  compute_gs  compute_track  \\\n",
      "0          576.0  3E0F76_969_2017_08  0.000000   10.327583      270.00003   \n",
      "1          320.0  3E0F76_969_2017_08  0.002869   10.327583      270.00003   \n",
      "2           64.0  3E0F76_969_2017_08  0.002869    0.000000      180.00000   \n",
      "3           64.0  3E0F76_969_2017_08  0.011918   32.575943      287.99356   \n",
      "4         -192.0  3E0F76_969_2017_08  0.011918    0.000000      180.00000   \n",
      "\n",
      "  month  time_diff  \n",
      "0     8        NaN  \n",
      "1     8        1.0  \n",
      "2     8        1.0  \n",
      "3     8        1.0  \n",
      "4     8        1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert timestamp to datetime if not already done\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'])\n",
    "\n",
    "# Extract year and month in the format YYYY_MM\n",
    "year_month = merged_data['timestamp'].dt.strftime('%Y_%m')\n",
    "\n",
    "# Modify the flight_id column\n",
    "merged_data['flight_id'] = merged_data['flight_id'] + '_' + year_month\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "168774d4-452c-4068-a00c-aa1a2ef7296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.sort_values(by=['flight_id', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a32b3fcb-7b95-4ad7-8a06-4314d953e986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                   flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
       "101055   3DD7B9_007_2017_09       0.0        -50.0         44.0  53.504489   \n",
       "101056   3DD7B9_007_2017_09       0.0        -50.0         44.0  53.504489   \n",
       "101057   3DD7B9_007_2017_09       0.0        -50.0         44.0  53.504489   \n",
       "101058   3DD7B9_007_2017_09     -25.0        -50.0         44.0  53.504489   \n",
       "101059   3DD7B9_007_2017_09     -25.0        -50.0         44.0  53.504489   \n",
       "...                     ...       ...          ...          ...        ...   \n",
       "1832681  RDF29_2110_2021_07    5050.0       5550.0         89.0  51.491547   \n",
       "1832683  RDF29_2110_2021_07    5025.0       5500.0         90.0  51.491318   \n",
       "1832685  RDF29_2110_2021_07    5000.0       5450.0         91.0  51.491197   \n",
       "1832687  RDF29_2110_2021_07    4925.0       5375.0         91.0  51.490825   \n",
       "1832689  RDF29_2110_2021_07    4900.0       5350.0         91.0  51.490677   \n",
       "\n",
       "         longitude  vertical_rate  compute_gs  \n",
       "101055   10.170916         -256.0         NaN  \n",
       "101056   10.170916         -256.0    0.000000  \n",
       "101057   10.170916         -256.0    0.000000  \n",
       "101058   10.170916         -256.0    0.000000  \n",
       "101059   10.170916         -256.0    0.000000  \n",
       "...            ...            ...         ...  \n",
       "1832681   9.160371        -2240.0   78.208565  \n",
       "1832683   9.159777        -2368.0   94.224470  \n",
       "1832685   9.159546        -2432.0   40.602660  \n",
       "1832687   9.158630        -2432.0  147.530010  \n",
       "1832689   9.158366        -2496.0   47.926025  \n",
       "\n",
       "[2177006 rows x 8 columns]>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the columns to keep as the input changes\n",
    "columns_to_keep = ['flight_id', 'altitude', 'geoaltitude', 'groundspeed', 'latitude', 'longitude', 'vertical_rate', 'compute_gs']\n",
    "df = merged_data[columns_to_keep]\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3bd53f3a-1b4e-4100-ac4c-4fc57c59ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_id            0\n",
      "altitude         13916\n",
      "geoaltitude      26632\n",
      "groundspeed      12173\n",
      "latitude             0\n",
      "longitude            0\n",
      "vertical_rate    12306\n",
      "compute_gs        1383\n",
      "dtype: int64\n",
      "flight_id        0.000000\n",
      "altitude         0.639227\n",
      "geoaltitude      1.223331\n",
      "groundspeed      0.559162\n",
      "latitude         0.000000\n",
      "longitude        0.000000\n",
      "vertical_rate    0.565272\n",
      "compute_gs       0.063528\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# finding the missing value and percentage of data\n",
    "# Summarize missing data\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary)\n",
    "# Percentage of missing values\n",
    "print((missing_summary / len(df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd984573-58fc-4d5e-8d9a-876593d72b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_67/1179182108.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna(\n",
      "/tmp/ipykernel_67/1179182108.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col] = df[col].fillna(method=\"ffill\")\n",
      "/tmp/ipykernel_67/1179182108.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna(method=\"ffill\")\n",
      "/tmp/ipykernel_67/1179182108.py:24: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col] = df[col].fillna(method=\"bfill\")\n",
      "/tmp/ipykernel_67/1179182108.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].fillna(method=\"bfill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total NaN counts per column:\n",
      "Feature 'flight_id':\n",
      "  NaN after imputation: 0\n",
      "Feature 'altitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'geoaltitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'groundspeed':\n",
      "  NaN after imputation: 0\n",
      "Feature 'latitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'longitude':\n",
      "  NaN after imputation: 0\n",
      "Feature 'vertical_rate':\n",
      "  NaN after imputation: 0\n",
      "Feature 'compute_gs':\n",
      "  NaN after imputation: 0\n",
      "\n",
      "Imputation completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define constants\n",
    "window_size = 10  # Window size for moving average\n",
    "\n",
    "# Initialize total NaN counts dictionaries\n",
    "total_missing_before = df.isnull().sum()\n",
    "total_missing_after = {col: 0 for col in df.columns}  # Initialize for all columns\n",
    "\n",
    "# Select only numeric columns for rolling mean\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Process each column\n",
    "for col in numeric_cols:\n",
    "    # Apply moving average to impute missing values\n",
    "    df[col] = df[col].fillna(\n",
    "        df[col].rolling(window=window_size, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Forward fill for remaining NaNs\n",
    "    df[col] = df[col].fillna(method=\"ffill\")\n",
    "    \n",
    "    # Backward fill for remaining NaNs\n",
    "    df[col] = df[col].fillna(method=\"bfill\")\n",
    "    \n",
    "    # Update total NaN counts after imputation\n",
    "    total_missing_after[col] = df[col].isnull().sum()\n",
    "\n",
    "# Handle non-numeric columns: update total_missing_after for consistency\n",
    "for col in df.columns:\n",
    "    if col not in numeric_cols:\n",
    "        total_missing_after[col] = df[col].isnull().sum()\n",
    "\n",
    "# Print total NaN counts before and after imputation\n",
    "print(\"\\nTotal NaN counts per column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"Feature '{col}':\")\n",
    "    print(f\"  NaN after imputation: {total_missing_after[col]}\")\n",
    "\n",
    "# Save the imputed DataFrame if needed\n",
    "# df.to_csv(\"imputed_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nImputation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d35b5bc9-a30f-4915-8ae6-acc89488f9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_id        0\n",
      "altitude         0\n",
      "geoaltitude      0\n",
      "groundspeed      0\n",
      "latitude         0\n",
      "longitude        0\n",
      "vertical_rate    0\n",
      "compute_gs       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Summarize missing data\n",
    "missing_summary = df.isnull().sum()\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "870871b2-d822-4c4c-9118-1ce874f34853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique flight IDs: 5324\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique values in the 'flight_id' column\n",
    "num_unique_flight_ids = df['flight_id'].nunique()\n",
    "\n",
    "print(f\"Number of unique flight IDs: {num_unique_flight_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "478ff0c6-af87-42e4-8f34-b0e5d5151fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique flight IDs: 5324\n",
      "Updated DataFrame:\n",
      "                  flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
      "1832681  RDF29_2110_2021_07    5050.0       5550.0         89.0  51.491547   \n",
      "1832683  RDF29_2110_2021_07    5025.0       5500.0         90.0  51.491318   \n",
      "1832685  RDF29_2110_2021_07    5000.0       5450.0         91.0  51.491197   \n",
      "1832687  RDF29_2110_2021_07    4925.0       5375.0         91.0  51.490825   \n",
      "1832689  RDF29_2110_2021_07    4900.0       5350.0         91.0  51.490677   \n",
      "\n",
      "         longitude  vertical_rate  compute_gs  \n",
      "1832681   9.160371        -2240.0   78.208565  \n",
      "1832683   9.159777        -2368.0   94.224470  \n",
      "1832685   9.159546        -2432.0   40.602660  \n",
      "1832687   9.158630        -2432.0  147.530010  \n",
      "1832689   9.158366        -2496.0   47.926025  \n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique values in the 'flight_id' column\n",
    "num_unique_flight_ids = df['flight_id'].nunique()\n",
    "print(f\"Number of unique flight IDs: {num_unique_flight_ids}\")\n",
    "\n",
    "# Extract unique flight IDs\n",
    "all_unique_ids = df['flight_id'].unique()\n",
    "\n",
    "# If you want to save the mapping for later use:\n",
    "#print(\"\\nList of unique flight IDs:\")\n",
    "#print(all_unique_ids)\n",
    "\n",
    "\n",
    "print(\"Updated DataFrame:\")\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ef801f9-ae43-4578-be38-53c6fcc0ffa3",
   "metadata": {},
   "source": [
    "# Extract unique flight IDs\n",
    "all_unique_ids = df['flight_id'].unique()\n",
    "\n",
    "# Create a mapping from flight_id to integers\n",
    "global_id_mapping = {flight_id: idx + 1 for idx, flight_id in enumerate(all_unique_ids)}\n",
    "\n",
    "# Replace 'flight_id' with its mapped integer values in the DataFrame\n",
    "df['flight_id'] = df['flight_id'].map(global_id_mapping)\n",
    "\n",
    "print(\"Mapping of 'flight_id' to integers completed.\")\n",
    "print(\"Updated DataFrame:\")\n",
    "print(df.tail())\n",
    "\n",
    "# If you want to save the mapping for later use:\n",
    "#print(\"\\nMapping from flight_id to integers:\")\n",
    "#print(global_id_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61ac7911-7aa2-4dbf-b132-d80a9aaca329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
      "0  3DD7B9_007_2017_09     425.0        400.0        138.0  53.353683   \n",
      "1  3DD7B9_042_2017_08     550.0        525.0         99.0  53.411291   \n",
      "2  3DD7B9_073_2017_08    1200.0       1200.0         93.0  53.408295   \n",
      "3  3DD7B9_075_2017_08     950.0        950.0        126.0  53.315094   \n",
      "4  3DD7B9_080_2017_08     525.0        475.0         96.0  53.393188   \n",
      "\n",
      "   longitude  vertical_rate  compute_gs  \n",
      "0   9.804984            0.0         0.0  \n",
      "1   9.892381            0.0         0.0  \n",
      "2   9.891384          448.0         0.0  \n",
      "3   9.771005         -128.0         0.0  \n",
      "4   9.839669            0.0         0.0  \n",
      "               flight_id  altitude  geoaltitude  groundspeed   latitude  \\\n",
      "5319  DHXCB_1762_2021_07     775.0       1150.0        125.0  50.750931   \n",
      "5320  RDF17_1583_2021_07    2775.0       2975.0        115.0  51.370488   \n",
      "5321  RDF17_2108_2021_07    2575.0       2900.0         87.0  51.685043   \n",
      "5322  RDF29_2109_2021_07    1775.0       2125.0        115.0  51.385477   \n",
      "5323  RDF29_2110_2021_07    4900.0       5350.0         91.0  51.490677   \n",
      "\n",
      "      longitude  vertical_rate  compute_gs  \n",
      "5319   7.109045         -384.0  128.199650  \n",
      "5320   9.188614         -256.0    0.000000  \n",
      "5321   9.203351         -192.0    0.000000  \n",
      "5322   9.327774         -384.0  192.254440  \n",
      "5323   9.158366        -2496.0   47.926025  \n"
     ]
    }
   ],
   "source": [
    "#taking the last values\n",
    "df_last = df.groupby('flight_id').last().reset_index()\n",
    "\n",
    "df_last = df_last[df.columns]\n",
    "\n",
    "print(df_last.head())\n",
    "print(df_last.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c832a89-6f81-476c-b292-6501b741338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitude  geoaltitude  groundspeed   latitude  longitude  vertical_rate  \\\n",
      "0     425.0        400.0        138.0  53.353683   9.804984            0.0   \n",
      "1     550.0        525.0         99.0  53.411291   9.892381            0.0   \n",
      "2    1200.0       1200.0         93.0  53.408295   9.891384          448.0   \n",
      "3     950.0        950.0        126.0  53.315094   9.771005         -128.0   \n",
      "4     525.0        475.0         96.0  53.393188   9.839669            0.0   \n",
      "\n",
      "   compute_gs  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#flight_ids = df_last['flight_id'].values\n",
    "saved_flight_ids = df_last[\"flight_id\"].values  # Save all flight_id values in order\n",
    "df_last_scaled_df = df_last.drop(columns=['flight_id'])\n",
    "print(df_last_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c786351-3ea3-485f-8373-5969066f6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   altitude  geoaltitude  groundspeed  vertical_rate  compute_gs         x  \\\n",
      "0     425.0        400.0        138.0            0.0         0.0  0.588155   \n",
      "1     550.0        525.0         99.0            0.0         0.0  0.587204   \n",
      "2    1200.0       1200.0         93.0          448.0         0.0  0.587248   \n",
      "3     950.0        950.0        126.0         -128.0         0.0  0.588748   \n",
      "4     525.0        475.0         96.0            0.0         0.0  0.587548   \n",
      "\n",
      "          y         z  \n",
      "0  0.101645  0.802335  \n",
      "1  0.102403  0.802935  \n",
      "2  0.102400  0.802904  \n",
      "3  0.101388  0.801933  \n",
      "4  0.101906  0.802747  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 'latitude' and 'longitude' columns in degrees\n",
    "df_last_scaled_df['lat_rad'] = np.radians(df_last_scaled_df['latitude'])  # Convert latitude to radians\n",
    "df_last_scaled_df['lon_rad'] = np.radians(df_last_scaled_df['longitude'])  # Convert longitude to radians\n",
    "\n",
    "# Compute x, y, z coordinates\n",
    "df_last_scaled_df['x'] = np.cos(df_last_scaled_df['lat_rad']) * np.cos(df_last_scaled_df['lon_rad'])\n",
    "df_last_scaled_df['y'] = np.cos(df_last_scaled_df['lat_rad']) * np.sin(df_last_scaled_df['lon_rad'])\n",
    "df_last_scaled_df['z'] = np.sin(df_last_scaled_df['lat_rad'])\n",
    "\n",
    "# Drop intermediate radian columns if not needed\n",
    "df_last_scaled_df.drop(columns=['lat_rad', 'lon_rad','latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(df_last_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b0fd28be-efca-4aca-b9c9-1ab2cf4c40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (5324, 8)\n",
      "y shape: (5324, 4)\n"
     ]
    }
   ],
   "source": [
    "X = df_last_scaled_df.values  \n",
    "y = df_last_scaled_df.iloc[:, [0, 5, 6,7]].values \n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b26775cb-38a1-4fc9-ac97-cd5fed2e4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after expansion: (5324, 1, 8)\n",
      "y shape: (5324, 4)\n"
     ]
    }
   ],
   "source": [
    "# Identify the altitude column index\n",
    "altitude_index = 0  # Change this based on your dataset\n",
    "\n",
    "# Separate the altitude column from X (features)\n",
    "altitude_X = X[:, altitude_index].reshape(-1, 1)\n",
    "X_other = X[:, [i for i in range(X.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# Separate the altitude column from y (target)\n",
    "altitude_y = y[:, altitude_index].reshape(-1, 1)\n",
    "y_other = y[:, [i for i in range(y.shape[1]) if i != altitude_index]]\n",
    "\n",
    "# **Use the SAME scalers from training**\n",
    "# Scale the altitude feature in X to [0, 1]\n",
    "altitude_X_scaled = scaler_X_altitude.transform(altitude_X)\n",
    "\n",
    "# Scale the other features in X to [-1, 1]\n",
    "X_other_scaled = scaler_X_other.transform(X_other)\n",
    "\n",
    "# Scale the altitude in y to [0, 1]\n",
    "altitude_y_scaled = scaler_y_altitude.transform(altitude_y)\n",
    "\n",
    "# Scale the other columns in y to [-1, 1]\n",
    "y_other_scaled = scaler_y_other.transform(y_other)\n",
    "\n",
    "# **Recombine scaled altitude with other features**\n",
    "X = np.hstack([altitude_X_scaled, X_other_scaled])\n",
    "y = np.hstack([altitude_y_scaled, y_other_scaled])\n",
    "\n",
    "# **Expand dimensions for LSTM input (samples, time_steps, features)**\n",
    "X = np.expand_dims(X, axis=1)  # Shape becomes (samples, 1, features)  \n",
    "\n",
    "# Ensure y is a NumPy array\n",
    "y = y.values if isinstance(y, pd.Series) else y  \n",
    "\n",
    "# Print the shapes\n",
    "print(\"X shape after expansion:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2a7880e-a98f-4fd7-8466-0969a6dd1d5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64b6e011-aeab-4592-801b-3280f15e338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'row_wise_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over each row in X\n",
    "for idx in range(len(X)):  # Iterate through all rows in the dataset\n",
    "    # Initialize last_input with the correct shape for 7 features\n",
    "    last_input = np.zeros((1, 1, 8))  # Match the input shape of the model\n",
    "    last_input[0, 0, :] = X[idx, :]  # Populate all 7 features from the current row\n",
    "\n",
    "    # Predict 60 seconds into the future for this row\n",
    "    next_prediction = model.predict(last_input, verbose=0)  # Predict the 60-second value\n",
    "\n",
    "\n",
    "    # Save predictions with flight_id and predicted values\n",
    "    predictions.append({\n",
    "        \"predicted_altitude\": next_prediction[0][0],  # Predicted altitude\n",
    "        \"predicted_x\": next_prediction[0][1],  # Predicted latitude\n",
    "        \"predicted_y\": next_prediction[0][2],  # Predicted longitude\n",
    "        \"predicted_z\":next_prediction[0][3]\n",
    "        # Add other predicted features if necessary\n",
    "    })\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Restore the flight_id column\n",
    "predictions_df[\"flight_id\"] = saved_flight_ids  # Ensuring original order\n",
    "\n",
    "# Reorder columns for better readability\n",
    "predictions_df = predictions_df[[\"flight_id\", \"predicted_altitude\", \"predicted_x\", \"predicted_y\",\"predicted_z\"]]\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "#predictions_df.to_csv('row_wise_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'row_wise_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c113254-1121-4ca8-a51f-e567f34779da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_id</th>\n",
       "      <th>predicted_altitude</th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>predicted_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>DHXCB_1762_2021_07</td>\n",
       "      <td>0.016390</td>\n",
       "      <td>0.857765</td>\n",
       "      <td>0.208342</td>\n",
       "      <td>0.915500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>RDF17_1583_2021_07</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.829707</td>\n",
       "      <td>0.234337</td>\n",
       "      <td>0.922495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>RDF17_2108_2021_07</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.818747</td>\n",
       "      <td>0.233513</td>\n",
       "      <td>0.926338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>RDF29_2109_2021_07</td>\n",
       "      <td>0.019360</td>\n",
       "      <td>0.829945</td>\n",
       "      <td>0.236031</td>\n",
       "      <td>0.922442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>RDF29_2110_2021_07</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.824154</td>\n",
       "      <td>0.233758</td>\n",
       "      <td>0.924002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               flight_id  predicted_altitude  predicted_x  predicted_y  \\\n",
       "5319  DHXCB_1762_2021_07            0.016390     0.857765     0.208342   \n",
       "5320  RDF17_1583_2021_07            0.023532     0.829707     0.234337   \n",
       "5321  RDF17_2108_2021_07            0.022598     0.818747     0.233513   \n",
       "5322  RDF29_2109_2021_07            0.019360     0.829945     0.236031   \n",
       "5323  RDF29_2110_2021_07            0.034511     0.824154     0.233758   \n",
       "\n",
       "      predicted_z  \n",
       "5319     0.915500  \n",
       "5320     0.922495  \n",
       "5321     0.926338  \n",
       "5322     0.922442  \n",
       "5323     0.924002  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()\n",
    "predictions_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "072946b6-522f-43c9-812c-94a90a1fcb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            flight_id  rescaled_altitude  rescaled_x  rescaled_y  rescaled_z\n",
      "0  3DD7B9_007_2017_09         501.221863    0.588674    0.101244    0.802191\n",
      "1  3DD7B9_042_2017_08         546.921814    0.587596    0.101884    0.802918\n",
      "2  3DD7B9_073_2017_08         760.516846    0.587452    0.102029    0.802899\n",
      "3  3DD7B9_075_2017_08         679.531067    0.589071    0.101110    0.801818\n",
      "4  3DD7B9_080_2017_08         542.298767    0.587985    0.101595    0.802629\n",
      "               flight_id  rescaled_altitude  rescaled_x  rescaled_y  \\\n",
      "5319  DHXCB_1762_2021_07         897.906494    0.626472    0.078651   \n",
      "5320  RDF17_1583_2021_07        1812.141113    0.615380    0.096985   \n",
      "5321  RDF17_2108_2021_07        1692.525024    0.611046    0.096404   \n",
      "5322  RDF29_2109_2021_07        1278.135010    0.615474    0.098180   \n",
      "5323  RDF29_2110_2021_07        3217.462646    0.613184    0.096576   \n",
      "\n",
      "      rescaled_z  \n",
      "5319    0.775665  \n",
      "5320    0.782007  \n",
      "5321    0.785490  \n",
      "5322    0.781958  \n",
      "5323    0.783372  \n"
     ]
    }
   ],
   "source": [
    "# Identify column indices\n",
    "altitude_index = 1  # Change this if altitude is in a different position\n",
    "\n",
    "# Extract predicted values\n",
    "predicted_altitude = predictions_df[['predicted_altitude']].values  # Extract altitude as 2D array\n",
    "predicted_other = predictions_df[['predicted_x', 'predicted_y','predicted_z']].values  # Extract other features\n",
    "\n",
    "# **Perform inverse transformation**\n",
    "rescaled_altitude = scaler_y_altitude.inverse_transform(predicted_altitude)  # Rescale altitude (0,1 → original range)\n",
    "rescaled_other = scaler_y_other.inverse_transform(predicted_other)  # Rescale other features (-1,1 → original range)\n",
    "\n",
    "# **Replace the original columns with the rescaled values**\n",
    "predictions_df['rescaled_altitude'] = rescaled_altitude  # Add rescaled altitude\n",
    "predictions_df[['rescaled_x', 'rescaled_y','rescaled_z']] = rescaled_other  # Add rescaled lat/lon\n",
    "\n",
    "# **Optionally drop the old scaled prediction columns**\n",
    "predictions_df.drop(columns=['predicted_altitude', 'predicted_x', 'predicted_y','predicted_z'], inplace=True)\n",
    "\n",
    "# **Verify the results**\n",
    "print(predictions_df.head())\n",
    "print(predictions_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e30b38db-56d3-4cb9-a8ce-daae0733a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            flight_id  rescaled_altitude   latitude  longitude\n",
      "0  3DD7B9_007_2017_09         501.221863  53.339886   9.758679\n",
      "1  3DD7B9_042_2017_08         546.921814  53.409649   9.836763\n",
      "2  3DD7B9_073_2017_08         760.516846  53.407829   9.852880\n",
      "3  3DD7B9_075_2017_08         679.531067  53.304047   9.739500\n",
      "4  3DD7B9_080_2017_08         542.298767  53.381916   9.803019\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute latitude from z\n",
    "predictions_df['latitude'] = np.degrees(np.arcsin(predictions_df['rescaled_z']))\n",
    "\n",
    "# Compute longitude from x and y\n",
    "predictions_df['longitude'] = np.degrees(np.arctan2(predictions_df['rescaled_y'], predictions_df['rescaled_x']))\n",
    "\n",
    "# Drop x, y, z if not needed\n",
    "predictions_df.drop(columns=['rescaled_x', 'rescaled_y', 'rescaled_z'], inplace=True)\n",
    "\n",
    "# Display the DataFrame with restored lat/lon\n",
    "print(predictions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "58312dee-f7ab-4a50-ae41-7a7f71f3d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  flight_id_x           timestamp  latitude_x  longitude_x  altitude  \\\n",
      "0  3E0F76_969 2017-08-01 07:25:23           0            0         0   \n",
      "1  3E0F76_970 2017-08-01 08:19:52           0            0         0   \n",
      "2  3E0F76_972 2017-08-01 11:26:36           0            0         0   \n",
      "3  3E0F76_973 2017-08-01 12:00:52           0            0         0   \n",
      "4  3E0F76_974 2017-08-01 13:42:23           0            0         0   \n",
      "\n",
      "  year_month         flight_id_y  rescaled_altitude  latitude_y  longitude_y  \n",
      "0    2017_08  3E0F76_969_2017_08        1954.391846   48.903801    11.500815  \n",
      "1    2017_08  3E0F76_970_2017_08        1927.667969   48.992008    11.630751  \n",
      "2    2017_08  3E0F76_972_2017_08        1882.400879   48.894482    11.054614  \n",
      "3    2017_08  3E0F76_973_2017_08        1974.208740   48.903557    11.005315  \n",
      "4    2017_08  3E0F76_974_2017_08        1864.688965   48.721371    11.386103  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original flight ID order and timestamp\n",
    "original_order_df = pd.read_csv(\"/home/jovyan/data/predictions_format.csv\")\n",
    "\n",
    "# Convert 'timestamp' column to datetime format\n",
    "original_order_df['timestamp'] = pd.to_datetime(original_order_df['timestamp'])\n",
    "\n",
    "# Extract year and month in the format YYYY_MM\n",
    "original_order_df['year_month'] = original_order_df['timestamp'].dt.strftime('%Y_%m')\n",
    "\n",
    "# Create a new column 'flight_id_rename'\n",
    "original_order_df['flight_id_rename'] = original_order_df['flight_id'].astype(str) + '_' + original_order_df['year_month']\n",
    "\n",
    "# Merge predictions_df into original_order_df using 'flight_id_rename'\n",
    "merged_df = original_order_df.merge(\n",
    "    predictions_df, \n",
    "    left_on='flight_id_rename', \n",
    "    right_on='flight_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the 'flight_id_rename' column as it's no longer needed\n",
    "merged_df.drop(columns=['flight_id_rename'], inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "004a049e-5dcd-437a-91f5-019d21d13b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled predictions saved to 'rescaled_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to a CSV file\n",
    "merged_df.to_csv('rescaled_predictions.csv', index=False)\n",
    "\n",
    "print(\"Rescaled predictions saved to 'rescaled_predictions.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
